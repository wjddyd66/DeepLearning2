{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding 계층\n",
    "**입력층의 원핫 표현과 가중치 행렬 W_in의 곱 계산**을 해결하기 위한 Embedding 계층에 대해서 알아보자  \n",
    "\n",
    "\n",
    "**Embedding 계층에 대한 Model**  \n",
    "<div><img src=\"https://user-images.githubusercontent.com/24144491/59848679-b60f0980-93a0-11e9-9d19-238ce3a8b238.png\" height=\"250\" width=\"600\" /></div>\n",
    "즉 저번 Post에서 설명한 W는 각 행이 각 단어의 분산 표현에 해당한다. 와 Input Data의 One-Hot-Encoding을 생각하여 Input Data를 W의 해당 단어의 분산 표현으로서 나타낸다는 의미 이다.  \n",
    "\n",
    "\n",
    "위의 특징을 합쳐서 **Embedding 계층의 Forward는 결과적으로 단지 행렬(W)의 특정 행을 추출하는 방식으로 구현**할 수 있다.  \n",
    "\n",
    "\n",
    "위와 같은 Forward를 구성하였으므로 **BackWard의 경우에도 해당하는 특정 행에만 기울기를 전달하고 나머지는 0으로서 구현하면 된다.**  \n",
    "\n",
    "\n",
    "여기서 **Batch 처리**를 생각하게 되면 Backward의 경우 **같은 Index가 오는 경우에 값이 덮여씌워지는 현상**이 발생할 수 있기때문에 **더하기 연산으로 구현해야 한다.**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span>W_out</span>의 곱 계산**을 어떻게 해결할 것인지를 생각해 보자.  \n",
    "전체적인 아이디어는 위에서의 Embedding과 같다.  \n",
    "저번 Post에서 설명한 **U는 W와 마찬가지로 각 행이 각 단어의 분산 표현에 해당한다.** \n",
    "Embedding과 마찬가지로 곱 계산을 하는 것이 아닌 **해당하는 행을 그저 가져오면 된다는 의미**이다.  \n",
    "이렇게 가져온 행과 은닉층의 뉴런과의 내적으로 인하여 **<span>W_out</span>의 곱 계산**을 어떻게 해결할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward**  \n",
    "실질적인 Parameter를 사용하기 위해 LossFunction을 미분할 줄 알아야 한다.  \n",
    "Loss Function을 CrossEntropy를 사용하고 Activation Function은 Sigmoid가 된다.  \n",
    "Loss Function을 미분하는 과정은 아래와 같다.  \n",
    "<p>$$y = \\frac{1}{1+ exp(-x)}$$</p>\n",
    "<p>$$\\frac{\\partial{y}}{\\partial{x}} = y(1-y)$$</p>\n",
    "<p>$$L = -(tlogy + (1-t)log(1-y))$$</p>\n",
    "<p>$$\\frac{\\partial{L}}{\\partial{x}} = -\\frac{\\partial{tlogy}}{\\partial{x}} + \\frac{\\partial{tlog(1-t)log(1-y)}}{\\partial{x}}$$</p>\n",
    "<p>$$ = -t\\frac{1}{y}y(1-y) + (1-t)\\frac{1}{1-y}y(1-y)$$</p>\n",
    "<p>$$= -t(1-y) + (1-t)y$$</p>\n",
    "<p>$$= y-t$$</p>\n",
    "\n",
    "위의 과정만으로 Loss를 측정해서 학습을 진행할 경우 Sigmoid의 Parameter라 긍정적인 예인 \"say\"에 대해서만 학습하게 된다.  \n",
    "\n",
    "정확한 분류를 위해서는 긍정적인인 예 \"say\"에 대해서는 출력을 1에 가깝게 만들고 \"say\"외의 모든 단어에 대해서는 0 으로서 출력값이 나오게 학습을 하는것이 목표이다.  \n",
    "\n",
    "**모든 부정적 예를 대상으로 이진 분류를 학습시키게 되면 연산량이 많아지므로 일부 부정적인 예를 통하여 학습하는 것**이 원래 목표였던 연산량을 줄이는 것에대한 적합한 방향이다.  \n",
    "이러한 **적은 수의 분정적 예를 샘플링해 사용하는 것이 네거티브 샘플링** 기법이다.  \n",
    "최종적인 Loss는 이러한 정답인 Loss와 네거티브 Loss의 합이 된다.  \n",
    "위와 같이 **Sigmoid with Loss 계층**은 다음과 같이 구현 될 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None  #  # sigmoid의 출력\n",
    "        self.t = None  # 정답 데이터\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = (self.y - self.t) * dout / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모든 부정적 예를 대상으로 이진 분류를 학습시키게 되면 연산량이 많아지므로 일부 부정적인 예를 통하여 학습하는 것**이 원래 목표였던 연산량을 줄이는 것에대한 적합한 방향이다.  \n",
    "이러한 **적은 수의 분정적 예를 샘플링해 사용하는 것이 네거티브 샘플링** 기법이다.  \n",
    "최종적인 Loss는 이러한 정답인 Loss와 네거티브 Loss의 합이 된다.  \n",
    "\n",
    "**네거티브 샘플링의 샘플링 기법**  \n",
    "네거티브 샘플링을 어떻게 샘플링을 할 것인지도 하나의 문제이다.  \n",
    "자주 등장하는 단어를 많이 추출하고 드물게 등장하는 단어를 적게 추출하는 것이 Model의 성능을 좋게 만들 것이라는 것을 알 수 있다.(희소한 단어 자체는 나올 확률이 매우 적기 때문)  \n",
    "이러한 과정은 각 **확률에 대해 0.75를 곱함으로 인하여 출현확률이 낮은 확률은 올리고 너무 높은 확률은 낮추는 방향**으로 진행하여 출현확률이 낮은 단어를 버리지 않는 방식으로 진행한다.  \n",
    "\n",
    "**UnigramSampler**\n",
    "- corpus: 말뭉치\n",
    "- power: 각 확률에 곱할 인수\n",
    "- sample_size: 샘플링을 수행할 횟수\n",
    "\n",
    "**np.random.choice(words,size,replace,p)**\n",
    "- words: 랜덤하게 뽑을 집합\n",
    "- size: 샘플링할 size\n",
    "- replace: False시 반복 X\n",
    "- p: 각가의 Sampling될 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.config import GPU\n",
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "        if not GPU:\n",
    "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                p = self.word_p.copy()\n",
    "                target_idx = target[i]\n",
    "                p[target_idx] = 0\n",
    "                p /= p.sum()\n",
    "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "        else:\n",
    "            # GPU(cupy）로 계산할 때는 속도를 우선한다.\n",
    "            # 부정적 예에 타깃이 포함될 수 있다.\n",
    "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
    "                                               replace=True, p=self.word_p)\n",
    "\n",
    "        return negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4]\n",
      " [0 4]\n",
      " [3 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "corpus = np.array([0,1,2,3,4,1,2,3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus,power,sample_size)\n",
    "target = np.array([1,3,0])\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**최종적인 네거티브 Sampling with Loss 계층을 구현한 Code이다.**  \n",
    "\n",
    "아래 코드는 다음과 같은 과정으로 진행된 것이다.  \n",
    "1. **입력층의 원핫 표현과 가중치 행렬 W_in의 곱 계산**을 해결하기 위한 Embedding 계층\n",
    "2. **<span>W_out</span>의 곱 계산** 을 해결하기 위한 EmbeddingDot 계층(입력 Embedding + HiddenLayer Embedding)\n",
    "3. **BackPropagation**을 위한 **Sigmoid + CrossEntropy = Sigmoid With Loss 계층**\n",
    "4.  네거티브 샘플링의 샘플링을 위한 UnigramSampler\n",
    "5.  네거티브 샘플링 계층 NegativeSampleingLoss(Embedding Dot + UnigramSample + Sigmoid With Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        #sample_size만큼 부정적인 예가 생김\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        #각 계층은 부정적인 예 + 1(긍정적인 예)의 Size를 가져야함\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "        #각 매개변수와 기울기는 배열로서 저장\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "\n",
    "        # 긍정적 예 순전파\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "\n",
    "        # 부정적 예 순전파\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 최종적으로 얻은 네거티브 샘플링 계층을 활용하여 CBOW Model을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)  # Embedding 계층 사용\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "\n",
    "        # 모든 가중치와 기울기를 배열에 모은다.\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW Model Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 9295 | 시간 0[s] | 손실 4.16\n",
      "| 에폭 1 |  반복 3001 / 9295 | 시간 183[s] | 손실 2.64\n",
      "| 에폭 1 |  반복 6001 / 9295 | 시간 404[s] | 손실 2.38\n",
      "| 에폭 1 |  반복 9001 / 9295 | 시간 632[s] | 손실 2.27\n",
      "| 에폭 2 |  반복 1 / 9295 | 시간 654[s] | 손실 2.23\n",
      "| 에폭 2 |  반복 3001 / 9295 | 시간 880[s] | 손실 2.15\n",
      "| 에폭 2 |  반복 6001 / 9295 | 시간 1086[s] | 손실 2.09\n",
      "| 에폭 2 |  반복 9001 / 9295 | 시간 1273[s] | 손실 2.04\n",
      "| 에폭 3 |  반복 1 / 9295 | 시간 1294[s] | 손실 2.02\n",
      "| 에폭 3 |  반복 3001 / 9295 | 시간 1514[s] | 손실 1.94\n",
      "| 에폭 3 |  반복 6001 / 9295 | 시간 1735[s] | 손실 1.92\n",
      "| 에폭 3 |  반복 9001 / 9295 | 시간 1955[s] | 손실 1.90\n",
      "| 에폭 4 |  반복 1 / 9295 | 시간 1977[s] | 손실 1.89\n",
      "| 에폭 4 |  반복 3001 / 9295 | 시간 2196[s] | 손실 1.82\n",
      "| 에폭 4 |  반복 6001 / 9295 | 시간 2414[s] | 손실 1.81\n",
      "| 에폭 4 |  반복 9001 / 9295 | 시간 2631[s] | 손실 1.80\n",
      "| 에폭 5 |  반복 1 / 9295 | 시간 2653[s] | 손실 1.79\n",
      "| 에폭 5 |  반복 3001 / 9295 | 시간 2874[s] | 손실 1.72\n",
      "| 에폭 5 |  반복 6001 / 9295 | 시간 3092[s] | 손실 1.72\n",
      "| 에폭 5 |  반복 9001 / 9295 | 시간 3309[s] | 손실 1.72\n",
      "| 에폭 6 |  반복 1 / 9295 | 시간 3330[s] | 손실 1.72\n",
      "| 에폭 6 |  반복 3001 / 9295 | 시간 3549[s] | 손실 1.64\n",
      "| 에폭 6 |  반복 6001 / 9295 | 시간 3770[s] | 손실 1.65\n",
      "| 에폭 6 |  반복 9001 / 9295 | 시간 3985[s] | 손실 1.65\n",
      "| 에폭 7 |  반복 1 / 9295 | 시간 4007[s] | 손실 1.64\n",
      "| 에폭 7 |  반복 3001 / 9295 | 시간 4228[s] | 손실 1.58\n",
      "| 에폭 7 |  반복 6001 / 9295 | 시간 4446[s] | 손실 1.59\n",
      "| 에폭 7 |  반복 9001 / 9295 | 시간 4664[s] | 손실 1.59\n",
      "| 에폭 8 |  반복 1 / 9295 | 시간 4685[s] | 손실 1.59\n",
      "| 에폭 8 |  반복 3001 / 9295 | 시간 4906[s] | 손실 1.52\n",
      "| 에폭 8 |  반복 6001 / 9295 | 시간 5126[s] | 손실 1.54\n",
      "| 에폭 8 |  반복 9001 / 9295 | 시간 5346[s] | 손실 1.54\n",
      "| 에폭 9 |  반복 1 / 9295 | 시간 5368[s] | 손실 1.55\n",
      "| 에폭 9 |  반복 3001 / 9295 | 시간 5591[s] | 손실 1.47\n",
      "| 에폭 9 |  반복 6001 / 9295 | 시간 5814[s] | 손실 1.49\n",
      "| 에폭 9 |  반복 9001 / 9295 | 시간 6035[s] | 손실 1.49\n",
      "| 에폭 10 |  반복 1 / 9295 | 시간 6057[s] | 손실 1.50\n",
      "| 에폭 10 |  반복 3001 / 9295 | 시간 6278[s] | 손실 1.43\n",
      "| 에폭 10 |  반복 6001 / 9295 | 시간 6499[s] | 손실 1.44\n",
      "| 에폭 10 |  반복 9001 / 9295 | 시간 6719[s] | 손실 1.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 49552 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 49892 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 49552 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 49892 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e74d8b790342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mword_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPU\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mword_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_vecs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/project/Python/DeepLearning2/common/util.py\u001b[0m in \u001b[0;36mto_gpu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "# 학습을 위한 함수\n",
    "import numpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from common import config\n",
    "import pickle\n",
    "from common.util import clip_grads\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "from dataset import ptb\n",
    "\n",
    "# 하이퍼 파라미터 설정\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# 데이터 읽기 + target, contexts 만들기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "\n",
    "# 모델 등 생성 - CBOW or SkipGram\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model,optimizer)\n",
    "\n",
    "# 학습 시작\n",
    "trainer.fit(contexts, target, max_epoch, batch_size, eval_interval = 3000) # eval_interval=500\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나중에 사용할 수 있도록 필요한 데이터 저장\n",
    "word_vecs = model.word_vecs\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(params, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW Model 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " we: 0.75439453125\n",
      " i: 0.70263671875\n",
      " your: 0.61083984375\n",
      " anything: 0.60498046875\n",
      " anybody: 0.6025390625\n",
      "\n",
      "[query] year\n",
      " month: 0.86279296875\n",
      " week: 0.78125\n",
      " summer: 0.7744140625\n",
      " spring: 0.73291015625\n",
      " decade: 0.68310546875\n",
      "\n",
      "[query] car\n",
      " truck: 0.62158203125\n",
      " window: 0.61328125\n",
      " cars: 0.5673828125\n",
      " auto: 0.55078125\n",
      " luxury: 0.5498046875\n",
      "\n",
      "[query] toyota\n",
      " engines: 0.64501953125\n",
      " honda: 0.6103515625\n",
      " unocal: 0.60595703125\n",
      " ford: 0.60498046875\n",
      " seita: 0.60498046875\n",
      "--------------------------------------------------\n",
      "\n",
      "[analogy] king:man = queen:?\n",
      " woman: 5.5234375\n",
      " text: 5.01953125\n",
      " naczelnik: 4.7265625\n",
      " father: 4.671875\n",
      " a.m: 4.65625\n",
      "\n",
      "[analogy] take:took = go:?\n",
      " came: 4.26171875\n",
      " began: 4.18359375\n",
      " went: 4.17578125\n",
      " 're: 4.12109375\n",
      " eurodollars: 4.08984375\n",
      "\n",
      "[analogy] car:cars = child:?\n",
      " a.m: 6.22265625\n",
      " rape: 5.60546875\n",
      " children: 5.5078125\n",
      " incest: 4.76171875\n",
      " woman: 4.62890625\n",
      "\n",
      "[analogy] good:better = bad:?\n",
      " rather: 5.69140625\n",
      " more: 5.65625\n",
      " less: 5.27734375\n",
      " greater: 4.45703125\n",
      " faster: 3.873046875\n"
     ]
    }
   ],
   "source": [
    "from common.util import most_similar, analogy\n",
    "\n",
    "pkl_file = 'cbow_params.pkl'\n",
    "\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    word_vecs = params['word_vecs']\n",
    "    word_to_id = params['word_to_id']\n",
    "    id_to_word = params['id_to_word']\n",
    "\n",
    "# 가장 비슷한(most similar) 단어 뽑기\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n",
    "\n",
    "# 유추(analogy) 작업\n",
    "print('-'*50)\n",
    "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
