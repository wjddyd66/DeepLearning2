{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통계 기반 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "통계 기법을 살펴보면서 **말뭉치(corpus)**를 사용한다.  \n",
    "말뭉치란 대량의 텍스트 데이터 이다. 맹목적으로 수집된 텍스트 데이터가 아닌 자연어 처리연구나 애플리케이션을 염두에 두고 수집된 텍스트 데이터를 일반적으로 **말뭉치**라고 칭한다.  \n",
    "\n",
    "말뭉치 전처리 과정을 텍스트 데이터를 단어로 분할하고 그 분할된 단어들을 단어 ID 목록으로 변환하는 일 이다.  \n",
    "\n",
    "말뭉치 전처리 과정: Text를 단어로 자른 뒤 고유의 ID를 붙여주는 과정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 확인 You say goodbye and I say hello\n",
      "단어 확인 ['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello']\n",
      "id_to_word 확인 {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello'}\n",
      "word_to_id 확인 {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5}\n",
      "결과 확인 [0 1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello'\n",
    "print('text 확인', text)\n",
    "#소문자로 변경\n",
    "text = text.lower()\n",
    "#마침표 처리\n",
    "text = text.replace('.', ' .')\n",
    "#Text => Word\n",
    "words = text.split(' ')\n",
    "print('단어 확인',words)\n",
    "\n",
    "#말뭉치 전처리 과정 Vocab 사전 만드는 과정\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word\n",
    "        \n",
    "print('id_to_word 확인',id_to_word)\n",
    "print('word_to_id 확인',word_to_id)\n",
    "\n",
    "# Text 말뭉치 전처리 결과\n",
    "import numpy as np\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "print('결과 확인', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 과정을 한번에 해결하는 Method이다.  \n",
    "return 으로서 word_to_id, id_to_word, corpus를 Return하므로 위의 과정을 한번에 처리할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 선언한 Method를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus:  [0 1 2 3 4 1 5]\n",
      "word_to_id:  {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5}\n",
      "id_to_word:  {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello'}\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print('corpus: ',corpus)\n",
    "print('word_to_id: ',word_to_id)\n",
    "print('id_to_word: ',id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 전처리 과정 이후 단어를 Vector로서 표현하는 방식이 필요하다. 이러한 방식은 임베딩이라 불린다.  \n",
    "임베딩 사전 지식은 아래 링크를 참조하자.  \n",
    "<a href=\"https://wjddyd66.github.io/tensorflow/2019/08/29/Tensorflow-RNN.html\">임베딩 상세내용</a><br>\n",
    "이러한 수많은 임베딩 방식 중에 오늘 Post할 방법은 **분포 가설**이다.  \n",
    "**분포 가설 이란 단어의 의미가 Text에서의 맥락에서 유추 가능하다**라는 것 이다.  \n",
    "**문맥 이라는 개념은 아래 사진참고하자**  \n",
    "\n",
    "<div><img src=\"https://raw.githubusercontent.com/wjddyd66/wjddyd66.github.io/master/static/img/AI/95.PNG\" height=\"250\" width=\"600\" /></div>\n",
    "\n",
    "그림참조: <a href=\"https://www.slideshare.net/ssuser06e0c5/i-64267027\">홍배 김</a><br>\n",
    "즉 **문맥**이란 하나의 Text에서 **특정 단어를 중심에 둔 그 주변 단어**를 의미한다.  \n",
    "여기서 주변 단어와 특정 단어와의 거리를 Window Size라고 표현한다.  \n",
    "\n",
    "이러한 **분포 가설**을 활용하여 단어를 Vector로 만드는 방법 중 하나가 **동시 발생 행렬**이다.  \n",
    "즉 주변 단어를 세어서 얼만큼 분포되어있는지를 확인하여 빈도로 표현하고 이를 벡터로 나타낸 것이다.  \n",
    "아래 Text에 대한 **동시 발생 행렬**을 정의하여 보자.  \n",
    "**Text: you say goodbye and i say hello**  \n",
    "<link rel = \"stylesheet\" href =\"/static/css/bootstrap.min.css\">\n",
    "\n",
    "동시 발생 행렬 선언  \n",
    "<table class=\"table\">\n",
    "\t<tbody>\n",
    "\t<tr>\n",
    "\t\t<td></td>\n",
    "\t\t<td>you</td>\n",
    "\t\t<td>say</td>\n",
    "\t\t<td>goodbye</td>\n",
    "\t\t<td>and</td>\n",
    "\t\t<td>i</td>\n",
    "\t\t<td>hello</td>\n",
    "\t\t<td>.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>you</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>say</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>0</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>goodbye</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>and</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>i</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>hello</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>1</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>.</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>0</td>\n",
    "\t\t<td>1</td>\n",
    "\t\t<td>0</td>\n",
    "\t</tr>\n",
    "\n",
    "</tbody>\n",
    "</table>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[1 0 1 0 1 1 0]\n",
      "[0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "C = np.array([\n",
    "    [0,1,0,0,0,0,0],\n",
    "    [1,0,1,0,1,1,0],\n",
    "    [0,1,0,1,0,0,0],\n",
    "    [0,0,1,0,1,0,0],\n",
    "    [0,1,0,1,0,0,0],\n",
    "    [0,1,0,0,0,0,1],\n",
    "    [0,0,0,0,0,1,0],\n",
    "],dtype=np.int32)\n",
    "print(C[0])\n",
    "print(C[1])\n",
    "print(C[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 Method는 위와 같은 과정을 수행하는 Method이다.\n",
    "- corpus: 말뭉치\n",
    "- vocab_size: vocab_size\n",
    "- window_size: window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Text를 단어로서 만들고 그러한 단어를 Vector로서 표현할 수 있게 되었다.  \n",
    "이 Vector사이에 서로 얼만큼 유사한지는 **코사인 유사도**를 사용하게 된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코사인 유사도\n",
    "\n",
    "**코사인 유사도 식**  \n",
    "\n",
    "<p>$similarity(x,y) = \\frac{x y}{||x|| ||y||} = \\frac{x_1y_1 + ... + x_ny_n}{\\sqrt{x_1^2 + ... + x_n^2}\\sqrt{y_1^2 + ... + y_n^2}}$</p>\n",
    "\n",
    "위의 식은 -1 ~ 1의 값을 가지게 되고 각 값에 대한 그림은 아래와 같다.  \n",
    "\n",
    "<div><img src=\"https://wikidocs.net/images/page/24603/%EC%BD%94%EC%82%AC%EC%9D%B8%EC%9C%A0%EC%82%AC%EB%8F%84.PNG\" height=\"250\" width=\"600\" /></div><br>\n",
    "\n",
    "즉 각각의 성분이 얼만큼 유사한지를 계산하는 과정이다.  \n",
    "좀 더 자세히 알아보게 되면 벡터의 내적의 연산 공식을 알면 된다.  \n",
    "아래와 같은 두 백터가 존재한다고 하였을때  \n",
    "\n",
    "<div><img src=\"http://bbs.nicklib.com/files/attach/images/197/670/001/2e36fca9e5f92188623aaa960f769906_1.png\" height=\"250\" width=\"600\" /></div><br>\n",
    "<p>$\\vec{a} \\bullet \\vec{b}=|\\vec{a}| |\\vec{b}|cos\\theta$</p>\n",
    "<p>$|\\vec{a}| = \\sqrt{a_1^2 + a_2^2}, |\\vec{b}| = \\sqrt{b_1^2 + b_2^2}$</p>\n",
    "<p>$\\vec{a} \\bullet \\vec{b} = a_1b_1 + a_2b_2$</p>\n",
    "\n",
    "따라서 아래와 같은 식이 성립하게 된다.  \n",
    "\n",
    "<p>$cos\\theta = \\frac{a_1b_1 + a_2b_2}{\\sqrt{a_1^2 + a_2^2} \\sqrt{b_1^2 + b_2^2}}$</p>\n",
    "\n",
    "<span>$cos\\theta$ </span>를 그림으로 나타내면 아래와 같다.  \n",
    "\n",
    "<div><img src=\"http://bbs.nicklib.com/files/attach/images/197/670/001/874ac7138a261329ca9af6388e2936de.png\" height=\"250\" width=\"600\" /></div><br>\n",
    "\n",
    "위의 그림에서도 알 수 있듯이 서로 Vector간에 성분을 얼만큼 표현하는 지에 관한 그림이 나오므로 이 말은 **서로 얼만큼 같은 성분을 가지고 있냐?**와도 같은 의미가 된다.  \n",
    "\n",
    "위의 식을 Code로서 나타내면 아래와 같다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**eps**는 아주 작은 값으로서 혹시나 0으로 나누는 과정이 없기 위한 값 이다.  \n",
    "\n",
    "위의 함수를 활용하여 Text= \"You say goodbye and I say hello\"에서 You와 I의 유사도를 비교하는 과정이다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']]  # \"you\"의 단어 벡터\n",
    "c1 = C[word_to_id['i']]    # \"i\"의 단어 벡터\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 선언한 Method를 개선하여 검색어와 가장 비슷한 단어를 추출하는 Method를 만들어보자. \n",
    "- query: 검색어\n",
    "- word_to_id: 단어에서 단어 ID로의 딕셔너리\n",
    "- id_to_word: 단어 ID에서 단어로의 딕셔너리\n",
    "- word_matrix: 단어 벡터들을 한데 모은 행렬, 각 행에는 대응하는 단어의 벡터가 저장되어있다고 가정\n",
    "- top: 상위 몇 개까지 추출할지에 관한 Parameter\n",
    "- argsort(): 배열에 담긴 원소의 인덱스를 내림차순으로 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "most_similar('you',word_to_id,id_to_word,C,top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 단어 사이의 거리만으로서 판단하는 것은 매우 큰 문제점이 발생하게 된다.  \n",
    "영어에서의 **the**와같이 많이 나오는 수식어는 자주 출몰하게되어서 모든 단어와의 거리가 가까워 지게 되고 이로 인하여 많은 단어와의 상관성이 높에 나올 수 있다.  \n",
    "이러한 문제점을 해결한 방안이 **PMI**이다.  \n",
    "<br><br>\n",
    "\n",
    "#### PMI\n",
    "\n",
    "**PMI 식**  \n",
    "\n",
    "<p>$PMI(x,y) = log_2 \\frac{P(x,y)}{P(x)P(y)}$</p>\n",
    "\n",
    "위의 식을 살펴보게 되면 P(x)는 x가 말뭉치에 등장할 확률을 가르키고 P(x,y)는 x와 y가 동시에 일어날 확률을 나타낸다.  \n",
    "즉 x 와 y가 나오는 각 시점에서 둘의 단어가 같이 나오면 이 단어의 상관성이 높다고 판단하는 것 이다.  \n",
    "C(x)가 x의 등장횟수이고 말뭉치에 포함된 단어 수를 N이라 하면 PMI의 식은 다음과 같다.  \n",
    "**PMI 식**  \n",
    "\n",
    "<p>$log_2 \\frac{P(x,y)}{P(x)P(y)} = log_2 \\frac{\\frac{C(x,y)}{N}}{\\frac{C(x)}{N}\\frac{C(y)}{N}} = log_2 \\frac{C(x,y)N}{C(x)C(y)}$</p>\n",
    "\n",
    "위의 식에서의 문제는 P(x,y) = 0 인경우 <span>$log_20 =-\\inf$ </span>이므로 문제가 발생하게 된다. 위와 같은 문제를 해결한 것이 **PPMI**이고 아래 식과 같다.  \n",
    "**PPMI 식**\n",
    "\n",
    "<p>$PPMI(x,y) = max(0,PMI(x,y))$</p>\n",
    "\n",
    "위와 같은 식은 아래 Code로서 나타낼 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시 발생 행렬\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "def ppmi(C, eps = 1e-8):\n",
    "    #동시 발생 행렬과 같은 크기의 행렬\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    #말뭉치에 포함된 단어 수\n",
    "    N = np.sum(C)\n",
    "    #각 단어의 발생 횟수\n",
    "    S = np.sum(C, axis=0)\n",
    "    \n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            #PMI 식\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            #PPMI 식\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "    return M\n",
    "W = ppmi(C)\n",
    "\n",
    "#유효 자릿수를 3자리로 표시\n",
    "np.set_printoptions(precision=3)\n",
    "print('동시 발생 행렬')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 방법에도 문제가 있다.  \n",
    "**말뭉치의 어휘 수가 증가함에 따라 각 단어 벡터의 차원 수도 증가한다는 문제**이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "위와 같은 문제를 위하여 차원감소 방법중에 SVD를 사용한다.  \n",
    "SVD에 자세한 내용은 아래 링크 참조  \n",
    "<a href=\"https://wjddyd66.github.io/ai/2019/07/24/PCA.html\">SVD 자세한 내용</a>  \n",
    "위와 같은 식은 **numpy의 ilnalg Module**을 사용하여 구현한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시 발생 행렬\n",
      "[0 1 0 0 0 0 0]\n",
      "----------\n",
      "PPMI\n",
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "----------\n",
      "SVD\n",
      "[ 3.409e-01  0.000e+00 -1.205e-01 -3.886e-16 -9.323e-01 -1.110e-16\n",
      " -2.426e-17]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(W)\n",
    "\n",
    "print('동시 발생 행렬')\n",
    "print(C[0])\n",
    "print('-'*10)\n",
    "print('PPMI')\n",
    "print(W[0])\n",
    "print('-'*10)\n",
    "print('SVD')\n",
    "print(U[0])\n",
    "\n",
    "# 플롯\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PTB 데이터 셋을 활용, sklearn의 SVD를 활용하여 선언한 Method의 결과를 확인하여 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 수 계산 ...\n",
      "PPMI 계산 ...\n",
      "calculating SVD ...\n",
      "\n",
      "[query] you\n",
      " i: 0.700317919254303\n",
      " we: 0.6367185115814209\n",
      " anybody: 0.565764307975769\n",
      " do: 0.563567042350769\n",
      " 'll: 0.5127798318862915\n",
      "\n",
      "[query] year\n",
      " month: 0.6961644291877747\n",
      " quarter: 0.6884941458702087\n",
      " earlier: 0.6663320660591125\n",
      " last: 0.6281364560127258\n",
      " next: 0.6175755858421326\n",
      "\n",
      "[query] car\n",
      " luxury: 0.6728832125663757\n",
      " auto: 0.6452109813690186\n",
      " vehicle: 0.6097723245620728\n",
      " cars: 0.6032834053039551\n",
      " corsica: 0.5698372721672058\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.7585658431053162\n",
      " nissan: 0.7148030996322632\n",
      " motors: 0.6926157474517822\n",
      " lexus: 0.6583304405212402\n",
      " honda: 0.6350275278091431\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('동시발생 수 계산 ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('PPMI 계산 ...')\n",
    "W = ppmi(C)\n",
    "\n",
    "print('calculating SVD ...')\n",
    "try:\n",
    "    # truncated SVD (빠르다!)\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "                             random_state=None)\n",
    "except ImportError:\n",
    "    # SVD (느리다)\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
